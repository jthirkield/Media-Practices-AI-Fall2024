{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c00075-5497-484c-9449-75c14d8ae5de",
   "metadata": {},
   "source": [
    "## Hugging Face ðŸ¤—\n",
    "\n",
    "https://huggingface.co/\n",
    "\n",
    "This is essentially the GitHub of AI models. When it comes to using well-trained models in python, this is where to go. There are many different types of models for many different tasks, and specifically for different subsets of tasks as well. The church for talking face is learning how to use its interface in order to leverage the publicly available models.\n",
    "\n",
    "As we learned in the previous class, with the deep learning example, if a model is already trained, we can actually leverage it, and even specifically train it by training its higher-level layers (i.e. the ones that were trained last in the precess). \n",
    "\n",
    "For this week's experience, we are just going to learn how to explore and evaluate the different models on Hugging Face, and, just as importantly start to develop our vocabulary for the types of models that are out there, so that we can find the right types of models that we might need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51a874-cbb3-4029-9e33-fa8ed8d2e7f7",
   "metadata": {},
   "source": [
    "### How To Begin\n",
    "\n",
    "#### Logging In\n",
    "Make sure to set up an account on Hugging Face to be able to access their page.\n",
    "\n",
    "#### Tasks\n",
    "This page is particularly helpful for seeing what types of tasks are offered, as well as an introduction/documentation on how to use models in Python that are built for each tasks\n",
    "\n",
    "https://huggingface.co/tasks\n",
    "\n",
    "#### Models\n",
    "What do you have a better understanding of the tasks, you can look up models are available here:\n",
    "\n",
    "https://huggingface.co/models\n",
    "\n",
    "If you click on the task menu on the left you will get a list of different models that are available.\n",
    "\n",
    "Generally, I recommend sorting by most downloaded, although trending can be interesting as well.\n",
    "\n",
    "Furthermore, you can evaluate the models directly on the model page by testing it. I certainly encourage using that route, but for the most part, for this week's experiment, we will be testing these models in python.\n",
    "\n",
    "#### Documentation\n",
    "How do we do this? Hugging Face has quite good documentation, although there is a lot of it!\n",
    "\n",
    "https://huggingface.co/docs\n",
    "\n",
    "\n",
    "The next cell details these three approaches to testing models:\n",
    "\n",
    "https://huggingface.co/docs/hub/models-widgets\n",
    "https://huggingface.co/docs/hub/models-inference\n",
    "https://huggingface.co/docs/transformers/\n",
    "\n",
    "So! You need to install that...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922c88f-dc03-4942-9f5c-4ea4e39b6091",
   "metadata": {},
   "source": [
    "### Testing Models\n",
    "\n",
    "There are three main ways to test models, which is I we'll be doing this week.\n",
    "\n",
    "**1. On the Browser using Widgets** These are directly accessible when you click on a model in the model repository:\n",
    "\n",
    "https://huggingface.co/models\n",
    "\n",
    "Hey some, allow you to type in text, upload images, and just use the model. In some cases you need to share information with the company, in order to use it. In some cases, they are not available.\n",
    "\n",
    "https://huggingface.co/docs/hub/models-widgets\n",
    "\n",
    "**2. Using the Inference API** there are a couple ways to use this, often just using the code that you click near the witch it will work, and there's several more robust client paste way of doing this. (Demoed below.)\n",
    "\n",
    "Most importantly, for these, you need to set up an **API key**. These are unique identifier's that allow hugging face to track your number of requests. You are allow 1000 free requests per day.\n",
    "\n",
    "https://huggingface.co/docs/hub/models-inference\n",
    "\n",
    "**3. Using the Transformers library's Pipeline** this is the most powerful way to do this for free. \n",
    "\n",
    "The main issue is that when you run this, you will actually download the entire model to your computer. For this reason, I would advise if you're using Google Colab generally use the Inference API to avoid the amount of downloads. But you can certainly do this on Colab.\n",
    "\n",
    "You can use this to test models, and there are no limits because you're downloading the model itself. You can also use this to fine-tune and train models (I will address that later in the semester.)\n",
    "\n",
    "https://huggingface.co/docs/transformers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412c25a-07b3-4eb1-a0d1-a390e537ab26",
   "metadata": {},
   "source": [
    "### Installations\n",
    "\n",
    "If you're going to use Google Colab to do this, there are likely no installations necessary. You will get a warning that asked for an API key that you need to hide on Google. Here is a guide for that:\n",
    "\n",
    "https://drlee.io/how-to-use-secrets-in-google-colab-for-api-key-protection-a-guide-for-openai-huggingface-and-c1ec9e1277e0\n",
    "\n",
    "If you are plan to use this on your computer you will need to install a few things:\n",
    "\n",
    "**Hugging Face hub**:\n",
    "\n",
    "`pip install huggingface-hub`\n",
    "\n",
    "I also recommend running this next (it allows you to manage the hub directly on your command line, which is useful for deleting models that you've downloaded so they don't take up too much space):\n",
    "\n",
    "`pip install -U \"huggingface_hub[cli]\"`\n",
    "\n",
    "**Transformers library**:\n",
    "\n",
    "`pip install transformers`\n",
    "\n",
    "And then install **Pytorch**\n",
    "\n",
    "`pip install torch`\n",
    "\n",
    "Note, *PyTorch* is the other main Python library for machine learning/deep learning, *Tensorflow/Keras* is the other.\n",
    "\n",
    "While Keras he is actually more readily learnable, generally speaking, PyTorch is beginning to dominate the field, because most researchers prefer it. And most importantly, the vast majority of the models on Hugging Face are more often compatible with PyTorch. (This one of the reasons why I did not ask us to install Tensorflow on our computers, it is better to have PyTorch there for this use.) \n",
    "\n",
    "Here is an interesting article on PyTorch vs Tensorflow:\n",
    "\n",
    "https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2023/\n",
    "\n",
    "**Note:** you may run into some conflicts/issues with your installations. When you do this, there might be a need to install other things as well, but hopefully you will get far enough with just these libraries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76979a-b5b9-45e3-8b33-c1907a5a6507",
   "metadata": {},
   "source": [
    "This cell just makes sure you've got PyTorch working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbb146-c9ea-42e1-b412-27e0a9937982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c2dfe-77f8-48a7-ab03-21400fb36f47",
   "metadata": {},
   "source": [
    "#### Inference API\n",
    "This allows you to send a request hunting a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06dcd7-7a26-4e4a-bb43-429f0e4f8924",
   "metadata": {},
   "source": [
    "This is an Image-to-Text Generation using Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b76d3-fac9-48ae-883d-49175b5b6a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-3.5-large-turbo\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"} #this is my API key\n",
    "#NEVER SHOW an API key publicly (and please do not use mine!)\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.content\n",
    "image_bytes = query({\n",
    "\t\"inputs\": \"View of the first circle of Dante's Inferno from the gates\",\n",
    "})\n",
    "# You can access the image with PIL.Image for example\n",
    "import io\n",
    "from PIL import Image\n",
    "image = Image.open(io.BytesIO(image_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696609db-e9aa-48d8-833a-1ef741ec149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d5ba9-3ac3-48f2-9e4c-8b4a8f9a4552",
   "metadata": {},
   "source": [
    "Trying this on Meta's Llama LLM\n",
    "\n",
    "It is too big, and you need more access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ca915-9f09-4319-93da-889a3a8798fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-405B\"\n",
    "headers = {\"Authorization\": \"Bearer DONTPUTAPIKEYSORPASSWORDSONGITHUB\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe15a8-8268-4cc5-a0d8-9fc8baeb9f55",
   "metadata": {},
   "source": [
    "#### Inference API with huggingface_hub\n",
    "\n",
    "Below, I'm using one of their examples using a \"Fill Mask\" model (sentence completion)\n",
    "\n",
    "This model is a little involved, but can make batch queries easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356efda2-d3d9-4ecc-b577-fa1487886301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(model=\"bert-base-uncased\", token=\"DONTPUTAPIKEYSORPASSWORDSONGITHUB!\")\n",
    "response = client.post(json={\"inputs\": \"The goal of life is [MASK].\"}, model=\"bert-base-uncased\")\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e15a8f-244b-4b33-a123-0f97d2913cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9bd98c7-c712-4172-a921-10ae24feb415",
   "metadata": {},
   "source": [
    "### Transformers/Pipeline Examples\n",
    "\n",
    "Each of these is essentially just adopted from Hugging faces documentation.\n",
    "\n",
    "Again, we will just be evaluating models and different types of categories that interest us.\n",
    "\n",
    "A lot of warnings may crop upâ€”transformers really loves to warn!! You can ignore many of them, or look into what they mean. I things should run fine. Here are some:\n",
    "\n",
    "\"Hardware accelerator e.g. GPU\" if you don't have a GPU it will show this. No worries.\n",
    "\n",
    "\"Some weights of the model checkpoint ... were not used when initializing\" That's fine.\n",
    "\n",
    "\"Neither max_length nor max_new_tokens has been set, max_length will default to 20\" You can set this in as a parameter when you load the model (max_new_tokens=200 0r max_length= 60)\n",
    "\n",
    "Further info on that: https://discuss.huggingface.co/t/confused-about-max-length-and-max-new-tokens/30892\n",
    "\n",
    "Each time you use a new model, your computer will take some time to download it. Some of these models are in the high 100MBs and many are above a GB. Once you download it they will be cached and you can continue to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70221c37-bf2a-4485-921c-c10ce0bc612f",
   "metadata": {},
   "source": [
    "#### Zero-Shot-Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9aa16-912f-4dc6-bea0-fc42d802d046",
   "metadata": {},
   "source": [
    "### Token Classification (NER Named Entity Recognition) \n",
    "\n",
    "https://huggingface.co/dslim/bert-base-NER?library=transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c64dc-de2c-4fc1-9aa1-313fa34495cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"\"\"Still, that hasnâ€™t stopped political operatives from both sides â€“ but especially the GOP â€“ from insisting that the numbers are good for them and devastating to the other side. And Republicans do have some reasons to be feeling good. In Nevada, for example, 24,000 more registered Republicans have voted than Democrats as of Friday in a state where Democratic strength in the powerful culinary unions has been thought to give them an edge â€“ and where recent elections have seen Democrats lead early voting. In Arizona, Republicans are outperforming Democrats 42%-36% among the early vote â€” another reversal of expectations and trends. In North Carolina the GOP has a razor-thin lead, where the conventional wisdom would have had Democrats meaningfully ahead. And even in Pennsylvania where Democrats have a double-digit advantage in returned ballots, Republicans have still made massive gains over their 2022 and 2020 performances in the early vote. (Michigan and Wisconsin donâ€™t offer party registration breakdowns for early voting, but there is fuzzier equivalent game going on in those states analyzing county data.)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "mytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec07122-6e99-461b-b15b-717fb855e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_pipe = pipeline(\"token-classification\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27b622-a890-4f55-8011-13a143d3f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ner_pipe(mytext)\n",
    "for entry in results:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961559aa-50f6-44cb-bcfd-14d8be396d28",
   "metadata": {},
   "source": [
    "#### Image to Text\n",
    "https://huggingface.co/Salesforce/blip-image-captioning-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4673a7-2a1e-476f-9180-39e37db3ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "imtext_pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\", max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88035817-4561-4d81-a438-56c80bb361e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "image = PIL.Image.open(\"images/scene1.jpg\")\n",
    "\n",
    "image.size[0]\n",
    "img = image.resize((int(image.size[0]/2), int(image.size[1]/2)))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf5991-c9c0-4130-95e1-9b979965980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imtext_pipe(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c77ee-ea73-4ab9-a0b7-114ea86d29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(\"images/pandas.jpg\")\n",
    "# image.show()\n",
    "imtext_pipe(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a8b22-5f67-444a-b1a7-63d536bbef62",
   "metadata": {},
   "source": [
    "#### Zero Shot Object Detection\n",
    "\n",
    "https://huggingface.co/google/owlvit-base-patch32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d90e6-9798-4cad-89b2-a1959d245e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "detector = pipeline(model=\"google/owlvit-base-patch32\", task=\"zero-shot-object-detection\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c38c2-7bb3-49c4-aca1-04f5989cab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"images/animal2.jpg\").convert(\"RGB\")\n",
    "\n",
    "predictions = detector(\n",
    "    image,\n",
    "    candidate_labels=[\"cat\", \"dog\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c062f-5250-4320-b40b-8fdb93d2fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf89d7-3e12-49c8-b71d-48b66b92fe17",
   "metadata": {},
   "source": [
    "#### Translation\n",
    "https://huggingface.co/tasks/translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f167f52-4fbc-4532-9658-7f3a53f23a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "en_fr_translator(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997a6f4-d10d-4d52-9005-7b14e195323d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a311708-a499-4335-9e39-e0642cad5dee",
   "metadata": {},
   "source": [
    "#### Sentence Similarity\n",
    "\n",
    "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f3d4f-f533-45a9-9c9c-6191d614c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897d902-4f6a-4e5d-be2a-b173a603246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentences = [\"Alfred gathered sea shells during his holiday\", \"Jane like to collect objects from nature.\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embedding_1= model.encode(sentences[0], convert_to_tensor=True)\n",
    "embedding_2 = model.encode(sentences[1], convert_to_tensor=True)\n",
    "\n",
    "util.pytorch_cos_sim(embedding_1, embedding_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cb6af-67a3-4eec-ade6-a2aa7618456e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8619ec-1217-4194-9731-bf8a1bbb20a6",
   "metadata": {},
   "source": [
    "#### Zero-Shot-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d5b33-1989-4f77-a464-056554d5241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "labels = [\"happy\",\"cats\",\"food\",\"vacation\"]\n",
    "pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "pipe(\"My kitten's birthday is today!\",\n",
    "     candidate_labels=labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34db3b-3978-4d69-858b-d6c881505a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe(\"I love my phone!\",\n",
    "     candidate_labels=labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b402913-6f67-4d17-af45-9f8698eec290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "output = query({\n",
    "    \"inputs\": \"My kitten's birthday is today!\",\n",
    "    \"parameters\": {\"candidate_labels\": [\"happy\",\"cats\",\"food\",\"vacation\"]},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d2fa96-9d3d-4b17-84c8-2fc68dd15bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40820ace-3c1b-4498-9f37-d79feeae0e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
