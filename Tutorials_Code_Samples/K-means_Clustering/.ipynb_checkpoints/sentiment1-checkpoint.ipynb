{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6053c01-f625-4910-aadd-bab6cd92ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk textblob "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c00075-5497-484c-9449-75c14d8ae5de",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "This is a long tradition in machine learning and natural language processing (NPL). Trying to computationally evaluate the sentiment of text. This often is scored of positive, neutral, and negative, and those are the ones that we will play with this week. (There are more sophisticated models out there that evaluate for different emotions, etc. \n",
    "\n",
    "Note: as usual, a human reader will be much better at assessing sentiment of anything written in their language(s). This is most often used for very large corpus of text as a way of essentially creating an advanced search.\n",
    "\n",
    "We will be playing with ML-based Natural Languages Processing libraries:\n",
    "\n",
    "**NLTK**: Natural Language Tool Kit https://www.nltk.org/ \n",
    "This has various applications for tokenizing languages, including libraries that can tag individual words for what kind of type they (verbs, proper nouns, etc) along with sentiment analysis tools.\n",
    "\n",
    "**TextBlob**: https://textblob.readthedocs.io/en/dev/\n",
    "this, in fact, interacts with NLTK read as well as pattern (that's a bit less easy to use) and has its own sentiment analysis approaches that we can evaluate as well.\n",
    "\n",
    "**Scapy**: https://spacy.io/\n",
    "This is a newer natural language library that has some powerful applications. We are going to brush by it (no need to install!) Because it's sentiment analysis module, which is relatively new is not significantly different from NLTK.\n",
    "\n",
    "Install the following:\n",
    "\n",
    "`pip install nltk`\n",
    "\n",
    "`pip install numpy` (dependency and useful!)\n",
    "\n",
    "\n",
    "`pip install -U textblob`\n",
    "\n",
    "`python -m textblob.download_corpora`\n",
    "\n",
    "The download_corpora might not actually work, but it's harmless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4a567-3591-4052-a068-7abb15f1bcb1",
   "metadata": {},
   "source": [
    "## Training\n",
    "The thing to understand about these libraries is that they were based on various training methods. Meaning, a large data set was produced in which documents or words were tagged with various levels of sentiment. So, depending on what they are trained on, they will be more or less useful, depending on what we want to evaluate.\n",
    "\n",
    "How they are trained:\n",
    "\n",
    "NLTK uses the **Vader** (Valence Aware Dictionary and sEntiment Reasoner) lexicon\n",
    "https://github.com/cjhutto/vaderSentiment\n",
    "\n",
    "Essentially different words are tagged with different levels of intensity scale from –4 to +4. \n",
    "\n",
    "\"okay\" = 0.9\n",
    "\n",
    "\"good\" = 1.9\n",
    "\n",
    "\"great\" = 3.1\n",
    "\n",
    "\"horrible\" = –2.5\n",
    "\n",
    "\":(\" = –2.2\n",
    "\n",
    "**Text Blob** uses a similar approach as Vader, but is based on **product reviews**\n",
    "\n",
    "**TextBlob's Naive Bayer Analyser** is based on machine learning algorithm trained on movie reviews.\n",
    "\n",
    "More on training later!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc2bcd-f609-437d-ab1d-f8c281fd4005",
   "metadata": {},
   "source": [
    "#### import NLTK and download all\n",
    "\n",
    "I would prefer to limit this to just a few downloads, as download all is going to download quite a bit and take a little time. But there is just too large of a chance of getting errors (actually in TextBlob) without downloading everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d228ad-6fba-4d02-b494-75c9dee3028d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/jonthirkield/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('vader_lexicon') #sentiment https://github.com/cjhutto/vaderSentiment\n",
    "# nltk.download('punkt') #this finds sentences in a text, not easy!\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8740e1-5086-4cf9-831d-ebf7442e66dc",
   "metadata": {},
   "source": [
    "#### using Vader \n",
    "You get four scores: pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d1131-9be1-4949-a434-da2340b559d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "# sia.polarity_scores(\"Excited about the upcoming weekend getaway!\")\n",
    "sia.polarity_scores(\"Traffic was terrible this morning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22624c-ff6d-4982-87d0-c9090d5f2a3d",
   "metadata": {},
   "source": [
    "compound score is the main one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7e0d2-f94d-4d42-b5cb-7e647886c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Savoring the flavors of a home-cooked meal. Simple joys are the heart of happiness.\"\n",
    "sia.polarity_scores(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71b6dd-7e77-4073-ad69-b1337538e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Spent hours creating the perfect playlist for every mood. Music is my therapy.\"\n",
    "sia.polarity_scores(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41eeca-a9f1-469b-a949-cfdde08095cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Celebrating a milestone at work! 🎉\"\n",
    "sia.polarity_scores(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fbf35-ce6c-4634-a435-69967ed9d6d2",
   "metadata": {},
   "source": [
    "$ pip install -U textblob\n",
    "$ python -m textblob.download_corpora\n",
    "https://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b2e1e-0732-45e4-8d16-815e2e52ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "# https://github.com/clips/pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49727ee7-d276-4118-bc7b-ca523c4d3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "output = TextBlob(\"Apple's name was inspired by Steve Jobs' visit to an apple farm while on a fruitarian diet.\")\n",
    "\n",
    "# Print out POS tagging\n",
    "output.tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44ec5b-242b-44f3-9007-fb9c89fe4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(\"Excited about the upcoming weekend getaway!\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159000f2-34d5-42ce-8209-eb93f3bbad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobber = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "blob = blobber(\"Excited about the upcoming weekend getaway!\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a489112-399d-4280-b811-d1fdf2e9dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frankenPh= \"\"\"These reflections have dispelled the agitation with which I began my\n",
    "letter, and I feel my heart glow with an enthusiasm which elevates me to\n",
    "heaven; for nothing contributes so much to tranquillize the mind as a\n",
    "steady purpose,—a point on which the soul may fix its intellectual eye.\n",
    "This expedition has been the favourite dream of my early years. I have\n",
    "read with ardour the accounts of the various voyages which have been\n",
    "made in the prospect of arriving at the North Pacific Ocean through the\n",
    "seas which surround the pole. You may remember, that a history of all\n",
    "the voyages made for purposes of discovery composed the whole of our\n",
    "good uncle Thomas’s library. My education was neglected, yet I was\n",
    "passionately fond of reading. These volumes were my study day and night,\n",
    "and my familiarity with them increased that regret which I had felt, as\n",
    "a child, on learning that my father’s dying injunction had forbidden my\n",
    "uncle to allow me to embark in a sea-faring life.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524edb2-6512-45a1-9392-ae4f2a3577ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = TextBlob(frankenPh)\n",
    "fn.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eaf659-9f8e-4b70-9489-d8964bbac29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74490491-f7c4-4516-83c5-84d8674da06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import asent\n",
    "\n",
    "# load spacy pipeline\n",
    "nlp = spacy.blank('en')\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "# add the rule-based sentiment model\n",
    "nlp.add_pipe('asent_en_v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c0f74-26b6-4e09-b064-1963211b21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try an example\n",
    "text = 'Why is everything so bad'\n",
    "#doc = nlp(text)\n",
    "\n",
    "# print polarity of document, scaled to be between -1, and 1\n",
    "print(doc._.polarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8ec7f-22c1-42bb-8c39-bd741d309c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65555138-648b-49ca-a024-ba66e25a3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c4301-9c9e-419b-8135-8760a06058e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "taggart_poem = \"\"\"To breathe and stretch one's arms again\n",
    "to breathe through the mouth to breathe to\n",
    "breathe through the mouth to utter in\n",
    "the most quiet way not to whisper not to whisper\n",
    "to breathe through the mouth in the most quiet way to\n",
    "breathe to sing to breathe to sing to breathe\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da9e4a-707c-42c7-b0a7-8dfd40d74c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(porter_stemmer.stem('challenging'))\n",
    "print(porter_stemmer.stem('vibes'))\n",
    "print(porter_stemmer.stem('reigns'))\n",
    "print(porter_stemmer.stem('tenderness'))\n",
    "print(porter_stemmer.stem('Overflowing'))\n",
    "print(porter_stemmer.stem('blessings'))\n",
    "print(porter_stemmer.stem('adoration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e54b489-309c-4462-9a4f-c88bd36e0e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc393cef-3e30-4893-a464-f17d6867d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "df = pd.DataFrame({'content': [\n",
    "    \"Just finished a challenging workout routine.\",\n",
    "    \"Political discussions heating up on the timeline.\",\n",
    "    \"Traffic was terrible this morning.\",\n",
    "    \"Enjoying a beautiful day at the park!\",\n",
    "    \"The new movie release is a must-watch!\",\n",
    "    \"Sending affectionate vibes to friends and family.\",\n",
    "    \"Overflowing adoration for a cute rescue puppy!\",\n",
    "    \"Confusion reigns as I try to make sense of recent events.\",\n",
    "    \"A moment of tenderness, connecting with loved ones.\",\n",
    "    \"Overflowing with gratitude for life's blessings\",\n",
    "]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3655597-7bcf-48da-b3c1-db3a3a0b1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts =   [\"Just finished a challenging workout routine.\",\n",
    "    \"Political discussions heating up on the timeline.\",\n",
    "    \"Traffic was terrible this morning.\",\n",
    "    \"Enjoying a beautiful day at the park!\",\n",
    "    \"The new movie release is a must-watch!\",\n",
    "    \"Sending affectionate vibes to friends and family.\",\n",
    "    \"Overflowing adoration for a cute rescue puppy!\",\n",
    "    \"Confusion reigns as I try to make sense of recent events.\",\n",
    "    \"A moment of tenderness, connecting with loved ones.\",\n",
    "    \"Overflowing with gratitude for life's blessings\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbe4b1-a9ce-4439-aa01-53280dada7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def make_stems(string_in):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", string_in).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english', tokenizer=make_stems)\n",
    "X = count_vectorizer.fit_transform(texts)\n",
    "print(count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d513a5-3379-4df0-85ab-b8099e5880c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(content):\n",
    "    blob = TextBlob(content)\n",
    "    nb_blob = blobber(content)\n",
    "    sia_scores = sia.polarity_scores(content)\n",
    "    spacy = nlp(content)\n",
    "    return pd.Series({\n",
    "        'content': content,\n",
    "        'textblob': blob.sentiment.polarity,\n",
    "        'textblob_bayes': nb_blob.sentiment.p_pos - nb_blob.sentiment.p_neg,\n",
    "        'nltk': sia_scores['compound'],\n",
    "        'spacy': spacy._.polarity.compound\n",
    "    })\n",
    "\n",
    "scores = df.content.apply(get_scores)\n",
    "scores.style.background_gradient(cmap='PiYG', axis=None, low=0.3, high=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686cf1d0-7a3e-4f93-9136-ea08bbc375c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a370146-037e-4270-9186-6dd7bd6c83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b336f2f-7780-4dfb-9bf2-d7e759ba1d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82468d2-4679-4531-a896-cf2c4954f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import asent\n",
    "\n",
    "# load spacy pipeline\n",
    "nlp = spacy.blank('en')\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "# add the rule-based sentiment model\n",
    "nlp.add_pipe('asent_en_v1')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46740e-5350-43b3-9f8a-0dcf0a4b19dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06467911-ff27-449b-b417-8c65551d8483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daeeca2-002c-41e9-b1ac-de6a51490fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca3a63-14be-458b-9ec0-2f30c0438a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sentiment140-subset.csv\", nrows=30000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87f037-e3a0-4406-ad92-a4f7de72e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a308ce5-7428-4832-ac00-ec3bf71f7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c8267-db71-42a5-8d4f-e3c38d4e52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0510bf-286f-4513-8e78-183532143713",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000, tokenizer=make_stems)\n",
    "vectors = vectorizer.fit_transform(df.text)\n",
    "words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe5a84-75b7-4e7b-8c80-dcf04dbeb026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac03e71-c864-41df-921a-e8b252caa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = words_df\n",
    "y = df.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0125d4c-19b8-483c-8f91-de246450ce56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ab102-4213-421a-8930-f102d06d49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dce6ee-e42d-498d-a43f-6e364b5524f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1cf66-a569-4f9b-a9c3-b1e148aee8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train a logistic regression\n",
    "logreg = LogisticRegression(C=1e9, solver='lbfgs', max_iter=1000)\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6dd89-7b5e-4827-ab41-0a0abfc59b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train a random forest classifier\n",
    "forest = RandomForestClassifier(n_estimators=50)\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cab818-a48f-4a56-88da-474dba32a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train a linear support vector classifier (LinearSVC)\n",
    "svc = LinearSVC()\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843274d9-35c9-439f-8fbe-0b370c0eac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train a multinomial naive bayes classifier (MultinomialNB)\n",
    "bayes = MultinomialNB()\n",
    "bayes.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8946becb-e8b4-4f0c-abd0-76cd95227a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "test_set = pd.DataFrame({'content': [\n",
    "   \"@kenbakernow is just about the coolest guy I know!!  Thanks kenny!\",\n",
    "    \"yum yum swedish fishies  mmm lets chat.haha\",\n",
    "    \"@Kbelize it should have also been stopped once legends such as janet, whitney &amp; prince jumped on bored. hurts my soul. \",\n",
    "    \"Today's the big day for the iPhone update in the UK.  Not ready yet though \",\n",
    "    \"&quot;Dear Sleep Diary, i'm sorry i've hurt your feelings by saying that you're imaginary. i'll make it up to you by buying you a new cover..&quot; \",\n",
    "    \"#Primeval won't return for a 4th season! DNW! \",\n",
    "    \"Going to bed. He's grounded for another week  this sucks i miss him really bad and now i cant even talk to him\",\n",
    "    \"@JonathanRKnight OMJ I go do the dishes and this is what I come back to... LOL I want a baby too \",\n",
    "    \"Supernatural me faz falta, Carry on my wayward son, there'll be peace when you are done \",\n",
    "    \"yah, todays gunna suck, prob be really busy and not on the computer much  | workin on a site w/ tabbed navigation, so far i have PS open\",\n",
    "    \"@TK2575 dude am i gonna miss your first recital ??????? \",\n",
    "    \"Bowl of Lucky Charms hit the spot.\"\n",
    "]})\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ffb01-d3dd-419a-a520-48e9f68d1bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a45a8-219f-454e-924b-489fee3e0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it through the vectoriser\n",
    "\n",
    "# transform, not fit_transform, because we already learned all our words\n",
    "unknown_vectors = vectorizer.transform(test_set.content)\n",
    "unknown_words_df = pd.DataFrame(unknown_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "unknown_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba76e6-ba91-4ddb-bbb1-b79b5d941be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae08e0d-09cc-4edc-9732-e4be315d67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression predictions + probabilities\n",
    "test_set['pred_logreg'] = logreg.predict(unknown_words_df)\n",
    "test_set['pred_logreg_proba'] = logreg.predict_proba(unknown_words_df)[:,1]\n",
    "\n",
    "# Random forest predictions + probabilities\n",
    "test_set['pred_forest'] = forest.predict(unknown_words_df)\n",
    "test_set['pred_forest_proba'] = forest.predict_proba(unknown_words_df)[:,1]\n",
    "\n",
    "# SVC predictions\n",
    "test_set['pred_svc'] = svc.predict(unknown_words_df)\n",
    "\n",
    "# Bayes predictions + probabilities\n",
    "test_set['pred_bayes'] = bayes.predict(unknown_words_df)\n",
    "test_set['pred_bayes_proba'] = bayes.predict_proba(unknown_words_df)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca354b9-e250-4acf-b35d-93fd71ba9b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ac506-4e32-4969-bca2-ecdff0928b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb3d0f-801d-4cf9-84ef-2e84426a81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53876d16-185c-4478-b8b5-7c74ead7a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training logistic regression\")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training random forest\")\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training SVC\")\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Naive Bayes\")\n",
    "bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2478a1b-0830-401a-bafc-e72019776f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177d59d-aba2-4e92-8a1d-04119bb7dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = logreg.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0628ad9-a4d3-4f5a-be3f-3de556069366",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = forest.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063c7a1-7acf-4bcd-b504-7e60b2db7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = svc.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cab035-bfb0-4d99-b2fe-0fc43376c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = bayes.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f028565-1bed-43d2-808f-917bf8614bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3c1ae-940d-4f91-b732-d4e33a8dcbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = logreg.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dbc05f-76ac-4005-9400-f11ec36ab807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55107b-a302-4101-af09-5f607d97d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = forest.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913c158-4173-4a77-97a2-5fd28a0975a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = svc.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6599ac-56c3-43a9-a7e6-e9753f8417fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = bayes.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56375a95-d7ca-44b6-a142-76968a76d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340856e-e05f-4049-9ee2-3b87c6832ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv(\"Shakespeare_data.csv\")\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be56f15-afed-4b61-9235-b7815b5d93ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs[dfs['Player'].notna()]\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04be9b2-971f-43ba-a60c-8cc5d62731de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs[dfs['ActSceneLine'].notna()]\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc054d77-062a-4ebc-a3e5-51c5b35f1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =dfs[dfs['Play'].isin([\"Hamlet\",\"Othello\",\"King Lear\"])]\n",
    "dfs[dfs['Play'].isin([\"Twelfth Night\",\"Much Ado about nothing\",\"As you like it\"])]\n",
    "# dfs[dfs['Play'].str.contains('As you')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c509c9e-655f-40e7-8fb3-25f42097e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist = [\"Hamlet\",\"Othello\",\"King Lear\",\"Twelfth Night\",\"Much Ado about nothing\",\"As you like it\"]\n",
    "trag_list = [\"Hamlet\",\"Othello\",\"King Lear\"]\n",
    "\n",
    "df2 =dfs[dfs['Play'].isin(playlist)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2740a68-a856-4e4b-9ea7-998f5077762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1a5ba-7904-4aed-9cfa-dfb94e87bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"score\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35811fe-5d42-44ae-a55d-39e59781fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d934b-ac94-44cc-8544-1b08408993bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2[\"Play\"].isin(trag_list), 'score'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aabbfd3-60d9-400f-9fe2-34195a80f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2[df2['Play'].isin(trag_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4d37c-711f-49c9-9cd7-d08ca25c7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df2[['score', 'PlayerLine']].copy()\n",
    "df_final.head()\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd21f0a-f383-4398-b8cf-321d28baff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.sample(frac=1).reset_index(drop=True)\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600dbc4-012f-4a34-9e2d-71e26addfcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f8d0d-9402-439a-b9ab-a747ad318ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('shakes_sentiment.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a68084-d9c5-4497-8224-69711dc5665b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff4675-f031-479a-8082-f83a4492980e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30145ac-6bad-4e75-98bb-3ffeb534d966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"shakes_sentiment.csv\", nrows=15000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfe89a-a94e-46fb-8320-d2cb76cadfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190b96d-7ac7-4e15-96ea-b0d5f7c9d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a5a63-28df-4747-ad9a-2730acfb3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e497d3-584f-4023-9e6f-88328928ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000, tokenizer=make_stems)\n",
    "vectors = vectorizer.fit_transform(df.PlayerLine)\n",
    "words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413bf85-a874-4f17-8015-a30e84e3c4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1caaf-5042-44c4-9427-2cd06cbf7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = words_df\n",
    "y = df.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbc051-6e10-4426-84c4-5f359373b9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520b697-8d67-47f8-930a-48a91d0a250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c2760-d1af-46b0-963a-4137aed6d776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238542f-ab9b-4d6a-89fa-065e73c59ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train a logistic regression\n",
    "logreg = LogisticRegression(C=1e9, solver='lbfgs', max_iter=1000)\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86548f-6609-46cc-a0b9-2a7692ad9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train a random forest classifier\n",
    "forest = RandomForestClassifier(n_estimators=50)\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed6af8-2ea6-4a23-8523-fcb7e194763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train a linear support vector classifier (LinearSVC)\n",
    "svc = LinearSVC()\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5099cf9-fa3e-46e1-bf2a-6fb2c1823956",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train a multinomial naive bayes classifier (MultinomialNB)\n",
    "bayes = MultinomialNB()\n",
    "bayes.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1063dd6-9f5c-414f-8700-85a66dcf5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "test_set = pd.DataFrame({'content': [\n",
    " \"Cassio came hither: I shifted him away,\",\n",
    "\"herself wittingly.\",\n",
    "\"Your answer, sir, is enigmatical:\",\n",
    "\"To spy into abuses, and oft my jealousy\",\n",
    "\"That we present us to him.\",\n",
    "\"Fit for the mountains and the barbarous caves,\",\n",
    "\"Merry, amen. I will, sir, I will.\",\n",
    "\"Starts up, and stands on end. O gentle son,\",\n",
    "\"Thou must be patient, we came crying hither:\",\n",
    "\"Can labour ought in sad invention,\",\n",
    "\"The same, my lord, and your poor servant ever.\",\n",
    "\"Nor scar that whiter skin of hers than snow,\",\n",
    "\"say her mind freely, or the blank verse shall halt\",\n",
    "\"If it be made of penetrable stuff,\",\n",
    "\"Are you good men and true?\",\n",
    "\"Benedick bear it, pluck off the bull's horns and set\",\n",
    "\"would have it at the Lady Hero's chamber-window.\",\n",
    "\"And every measure fail me.\",\n",
    "\"signior, walk aside with me: I have studied eight\",\n",
    "\"She shall be buried with her face upwards.\",\n",
    "\"Is he not jealous?\",\n",
    "\"Be not amazed, right noble is his blood.\",\n",
    "\"As of a father: for let the world take note,\",\n",
    "\"Traitors ensteep'd to clog the guiltless keel,--\",\n",
    "\"And with what wing the staniel cheques at it!\",\n",
    "\"care for her frowning, now thou art an O without a\",\n",
    "\"You will never run mad, niece.\",\n",
    "\"Hail to your grace!\",\n",
    "\"From Goneril his mistress salutations,\",\n",
    "\"What a piece of work is a man! how noble in reason!\"\n",
    "]})\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f77cd-8dc7-4271-af0a-3ebdbc620e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb802e-01fd-494f-b890-b305fb8020b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it through the vectoriser\n",
    "\n",
    "# transform, not fit_transform, because we already learned all our words\n",
    "unknown_vectors = vectorizer.transform(test_set.content)\n",
    "unknown_words_df = pd.DataFrame(unknown_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "unknown_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62123031-f7f0-4726-abbb-7e23bb84aad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b4799-92e4-49cf-9c5c-20329d6ca498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression predictions + probabilities\n",
    "test_set['pred_logreg'] = logreg.predict(unknown_words_df)\n",
    "test_set['pred_logreg_proba'] = logreg.predict_proba(unknown_words_df)[:,1]\n",
    "\n",
    "# Random forest predictions + probabilities\n",
    "test_set['pred_forest'] = forest.predict(unknown_words_df)\n",
    "test_set['pred_forest_proba'] = forest.predict_proba(unknown_words_df)[:,1]\n",
    "\n",
    "# SVC predictions\n",
    "test_set['pred_svc'] = svc.predict(unknown_words_df)\n",
    "\n",
    "# Bayes predictions + probabilities\n",
    "test_set['pred_bayes'] = bayes.predict(unknown_words_df)\n",
    "test_set['pred_bayes_proba'] = bayes.predict_proba(unknown_words_df)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aaf4dc-3d40-4a2f-958a-60c284bbc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec5fb0-c941-4cb4-b2c2-948964d34f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0be3f-f4e1-4e34-bc46-fa3a61eaa463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953012b6-e9e6-4707-9d31-88cf01b41c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training logistic regression\")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training random forest\")\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training SVC\")\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Naive Bayes\")\n",
    "bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21b2f4-dfdb-4913-b5e2-a615da21989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36899eaa-6070-41b2-aae3-bf6187c3d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = logreg.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db04584-5a46-4955-af45-90f5a4448ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = forest.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77347e0b-7029-4f74-8651-9856a8d8a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = svc.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5096e2-e41e-47c1-a710-34ec67d0e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = bayes.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361faea9-5332-4643-b7f5-b9691c6ed398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8139e0-fbaf-4530-bed0-e940466d5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = logreg.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a056b7-9386-4151-bd9a-5e6922faaf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3d762-48af-43ba-a2cd-2db929596dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = forest.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b547b-d55e-42f1-ae23-7cd460e5de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = svc.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffd688-ce1d-4f60-bb22-0c401518b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = bayes.predict(X_test)\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['negative', 'positive'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92f7d9-486a-4760-9905-d9f08ab61b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
